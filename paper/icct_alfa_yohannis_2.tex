\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}

\usepackage{graphicx}
\usepackage{subcaption}

\usepackage{url}
\usepackage[hidelinks]{hyperref}
\Urlmuskip=0mu plus 1mu
\usepackage{breakurl}

\usepackage{listings}
\lstdefinelanguage{bash} {
	keywords={},
	basicstyle=\ttfamily\small,
	keywordstyle=\color{blue}\bfseries,
	ndkeywords={},
	ndkeywordstyle=\color{purple}\bfseries,
	sensitive=true,
%	numbers=left,
	numberstyle=\tiny\color{gray},
	breaklines=true,
	frame=lines,
	backgroundcolor=\color{lightgray!10},
	tabsize=2,
	comment=[l]{\#},
	morecomment=[s]{/*}{*/},
	commentstyle=\color{gray}\ttfamily,
	stringstyle=\color{purple}\ttfamily,
	showstringspaces=false,
	morestring=[s]{"}{"}
}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Benchmarking PostgreSQL and MongoDB for Image-Based Time-Series Data\\
% {\footnotesize \textsuperscript{*}Note: Sub-titles are not captured in Xplore and
% should not be used
% }
% \thanks{Identify applicable funding agency here. If none, delete this.}
}

\author{
\IEEEauthorblockN{1\textsuperscript{st} Alfa Yohannis}
\IEEEauthorblockA{\textit{Department of Informatics} \\
\textit{Universitas Pradita}\\
Tangerang, Indonesia \\
alfa.ryano@gmail.com}
\and
\IEEEauthorblockN{2\textsuperscript{nd} Alexander Waworuntu}
\IEEEauthorblockA{\textit{Department of Informatics} \\
\textit{Universitas Multimedia Nusantara}\\
Tangerang, Indonesia}
\and
\IEEEauthorblockN{3\textsuperscript{rd} Master Edison Siregar}
\IEEEauthorblockA{\textit{Department of Informatics} \\
\textit{Universitas Pradita}\\
Tangerang, Indonesia}
}



%\author{
%\IEEEauthorblockN{
%Alfa Yohannis\IEEEauthorrefmark{1},
%Alexander Waworuntu\IEEEauthorrefmark{2},
%Master Edison Siregar\IEEEauthorrefmark{3}
%}
%\IEEEauthorblockA{
%\IEEEauthorrefmark{1}\IEEEauthorrefmark{3}\textit{Department of Informatics}\\
%\textit{Universitas Pradita}\\
%Tangerang, Indonesia\\
%\IEEEauthorrefmark{1}alfa.ryano@gmail.com
%}
%\IEEEauthorblockA{
%\IEEEauthorrefmark{2}\textit{Department of Informatics}\\
%\textit{Universitas Multimedia Nusantara}\\
%Tangerang, Indonesia
%}
%}




\maketitle

\begin{abstract}
This paper presents a performance comparison between PostgreSQL with TimescaleDB and MongoDB for image-based time-series workloads in consumer electronics and IoT systems. Using 320x240 JPEG frames, the study evaluates insertion throughput, aggregation latency, driver overhead, and storage utilization under controlled conditions. Experimental results show that MongoDB achieves higher ingestion throughput and significantly smaller storage footprint due to native compression, while PostgreSQL provides substantially lower aggregation latency and more stable analytical performance. These findings highlight clear trade-offs between write efficiency, query performance, and storage efficiency for vision-oriented time-series applications.
\end{abstract}

\begin{IEEEkeywords}
time-series databases, image-based IoT, PostgreSQL, MongoDB, consumer electronics
\end{IEEEkeywords}

\section{Introduction}
\label{sec:introduction}

The rapid proliferation of consumer electronics (CE) and Internet of Things (IoT) devices equipped with embedded vision capabilities has generated unprecedented volumes of image-based time-series data, ranging from security cameras and smart doorbells to industrial inspection systems and wearable health monitors~\cite{intuz_iot_databases,ceec_tsdb_review}. These systems produce continuous streams of timestamped image frames alongside metadata such as device identifiers and sensor parameters, placing unique demands on database management systems for high-throughput ingestion, efficient storage of binary payloads, and low-latency temporal analytics~\cite{jensen_tsms_survey,nerc_tsdb_survey}. Traditional relational databases struggle with the scale and velocity of such workloads, while specialized time-series databases (TSDBMSs) and document stores have emerged as viable alternatives, yet their comparative performance for image-centric time-series remains underexplored~\cite{dewaal_timeseries_survey,tigerdata_tsdb_compare}.

PostgreSQL extended with TimescaleDB and MongoDB represent two prominent architectural approaches for managing time-series data in production environments~\cite{influxdata_mongodb_timescaledb,tigerdata_mongodb_timescaledb}. TimescaleDB augments PostgreSQL’s relational foundation with hypertables, automatic time-based partitioning, and chunked storage optimized for temporal queries, while MongoDB leverages its document model and native time-series collections with WiredTiger compression for flexible schema handling and high-ingestion workloads~\cite{maddevs_timescaledb_beats_pg,queryleaf_mongodb_timeseries_iot}. Existing benchmarks primarily focus on scalar sensor metrics (e.g., temperature, CPU utilization) using tools such as the Time Series Benchmark Suite (TSBS), leaving a critical gap in understanding how these systems perform when handling realistic image payloads representative of CE vision streams~\cite{tsbs,cratedb_ts_benchmark,diva_timescaledb_mongodb_eval}.

This study addresses this gap through a systematic performance comparison of PostgreSQL with TimescaleDB versus MongoDB for image-based time-series workloads modeled after consumer electronics applications. The primary aim is to quantify trade-offs across key metrics---insertion throughput, aggregation latency, driver overhead, and storage efficiency---under controlled conditions using standardized $320 \times 240$ JPEG frames with accompanying metadata on commodity hardware. This work introduces a reproducible benchmarking methodology for image-based time-series data, provides empirical evidence of complementary strengths between relational time-series extensions and document stores, and offers actionable guidance for CE system architects in selecting databases based on ingestion rate requirements, analytical latency needs, and storage constraints in multi-camera deployments. These findings extend prior TSDBMS surveys by addressing multimedia time-series use cases that are increasingly prevalent in next-generation vision-enabled IoT systems~\cite{jensen_tsms_survey,dewaal_timeseries_survey}.




\section{Related Work}
\label{sec:related_work}

Time-series database management systems (TSDBMSs) have been extensively studied for workloads dominated by temporally ordered sensor and event data, particularly in IoT and DevOps contexts~\cite{dewaal_timeseries_survey,jensen_tsms_survey}. Prior surveys report that specialized engines and extensions, such as TimescaleDB on PostgreSQL, introduce time-based partitioning, chunking, and compression mechanisms to overcome the scalability and performance limitations of traditional row-oriented relational databases~\cite{dewaal_timeseries_survey,nerc_tsdb_survey,diva_timescaledb_mongodb_eval}. These studies consistently identify ingestion throughput, query latency, and storage footprint as the dominant performance metrics, which directly align with the dimensions evaluated in this work.

Several benchmarks and industrial reports directly compare TimescaleDB with general-purpose document stores such as MongoDB for time-series workloads~\cite{influxdata_mongodb_timescaledb,tigerdata_mongodb_timescaledb,tsbs}. Results commonly indicate that TimescaleDB’s hypertable abstraction and chunk-based storage provide competitive or superior ingestion performance and substantially faster time-based aggregations than MongoDB for metric-style data, especially for large device fleets and long time ranges~\cite{maddevs_timescaledb_beats_pg}. At the same time, MongoDB’s native time-series collections and document-oriented schema offer operational simplicity and flexible schema evolution, which remain attractive for heterogeneous IoT deployments despite higher aggregation overheads in some benchmarks~\cite{queryleaf_mongodb_timeseries_iot}.

In the consumer electronics and IoT domain, most prior studies focus on scalar sensor signals such as temperature, power, or motion rather than image-based time-series streams~\cite{intuz_iot_databases,ceec_tsdb_review}. Database selection guidelines often emphasize high ingest rates, retention policies, and compression, and frequently recommend TimescaleDB and MongoDB for time-stamped data, yet without addressing large binary payloads such as JPEG frames~\cite{emqx_mqtt_mongodb_benchmark}. Existing benchmarking tools, including TSBS, rely primarily on synthetic numeric workloads~\cite{tsbs,cratedb_ts_benchmark}, leaving a clear gap in understanding database behavior under image-centric workloads. The present study addresses this gap by evaluating PostgreSQL with TimescaleDB and MongoDB under identical binary-image ingestion, aggregation, and storage conditions, thereby extending TSDBMS evaluations to vision-oriented consumer electronics and embedded IoT systems~\cite{dewaal_timeseries_survey,nerc_tsdb_survey,jensen_tsms_survey}.





\section{Methodology}
\label{sec:methodology}

This study evaluates PostgreSQL with TimescaleDB and MongoDB for image-based time-series workloads representative of consumer electronics (CE) and IoT systems. The dataset consists of fixed-size image frames generated from a single source image resized to $320 \times 240$ pixels and encoded in JPEG format, reflecting the resolution of low-power embedded vision devices and entry-level camera sensors. Each frame is stored together with a device identifier, timestamp, image dimensions, and MIME type. To ensure controlled and repeatable experimental conditions, the same image is reused for all insert operations, eliminating variability caused by heterogeneous content.

In the PostgreSQL configuration, image frames are stored in a relational time-series table configured as a TimescaleDB hypertable. Each record includes a unique frame identifier, device identifier, high-resolution timestamp, raw image bytes stored as a binary attribute, image width and height, MIME type, and a creation timestamp. To satisfy TimescaleDB requirements, a composite primary key consisting of the timestamp and unique frame identifier is used, enabling correct partitioning and time-based chunk management for efficient temporal indexing and scalable storage.

In MongoDB, the same logical information is stored as documents in a native time-series collection. Each document contains a device identifier, timestamp, binary image payload stored as \texttt{BinData}, image width and height fields, and a MIME type field. MongoDB relies on the WiredTiger storage engine with native block-level compression applied to both data and indexes. In contrast, PostgreSQL uses uncompressed relational storage with TOAST mechanisms for handling large binary objects. Although the physical storage layouts differ, both systems store identical logical content to ensure a fair comparison of storage and performance behavior.

The benchmarking process consists of three phases: insertion throughput, aggregation performance, and driver roundtrip overhead. For insertion benchmarking, each measured run inserts $200{,}000$ image frames using a fixed batch size of $1{,}000$ records per transaction, following a warm-up phase to stabilize internal buffers and storage allocation. Each measured run starts from an empty table or collection to ensure identical initial conditions. Aggregation benchmarking uses fixed one-minute time-bucket queries to compute total frame counts and average image sizes, while driver overhead is measured using minimal roundtrip queries to isolate client--server communication latency independent of query complexity.

Four primary metrics are evaluated: insertion throughput (rows/sec), aggregation latency (ms), driver roundtrip latency (ms), and storage utilization (MB). PostgreSQL storage is measured using relation size functions and TimescaleDB hypertable size functions, while MongoDB storage is obtained using \texttt{collStats} and \texttt{dbStats}. All experiments are conducted on Ubuntu~22.04 with an Intel Core~i7 processor, 16~GB memory, and 4 physical cores (8 logical threads). The database stack consists of PostgreSQL~18.1 with TimescaleDB~2.24.0 and MongoDB~7.0.26. The benchmarking framework is implemented in Python using psycopg2~2.9.10 and PyMongo~4.15.4 with synchronous single-connection clients to isolate intrinsic database engine performance.


\begin{table}
\centering
\renewcommand{\arraystretch}{1.3}
\caption{Performance and Storage Comparison between PostgreSQL and MongoDB}
\label{tab:benchmark_summary}
\begin{tabular}{p{.18\textwidth}cc}
\hline
\textbf{Metric} & \textbf{PostgreSQL} & \textbf{MongoDB} \\
\hline

\multicolumn{3}{l}{\textbf{Insert Throughput (rows/sec)}} \\
PostgreSQL vs MongoDB  
& $1785.05 \pm 274.34$ 
& $3783.46 \pm 551.40$ \\

\hline
\multicolumn{3}{l}{\textbf{Aggregation Latency (ms)}} \\
PostgreSQL vs MongoDB  
& $23.97 \pm 0.82$ 
& $353.13 \pm 16.53$ \\

\hline
\multicolumn{3}{l}{\textbf{Driver Roundtrip Latency (ms)}} \\
PostgreSQL vs MongoDB  
& $0.0415 \pm 0.0360$ 
& $0.0858 \pm 0.0030$ \\

\hline
\multicolumn{3}{l}{\textbf{Storage Size (MB)}} \\
Table / Collection  
& $4777.72$ 
& $835.41$ \\

Database Total  
& $4787.29$ 
& $835.45$ \\

\hline
\end{tabular}
\end{table}


\begin{figure}
\centering

\begin{subfigure}{0.48\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/boxplot_insert_throughput.pdf}
    \caption{Insert Throughput}
    \label{fig:sub_insert_throughput}
\end{subfigure}
\hfill
\begin{subfigure}{0.48\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/boxplot_aggregation_latency.pdf}
    \caption{Aggregation Latency}
    \label{fig:sub_aggregation_latency}
\end{subfigure}

\caption{Performance comparison of PostgreSQL and MongoDB across insertion throughput and aggregation latency.}
\label{fig:benchmark_boxplots}
\end{figure}


\subsection{Insert Throughput Performance}
\label{subsec:insert_throughput}

Insert throughput is a key performance indicator for image-based time-series workloads, as it reflects the system’s ability to ingest continuous high-volume sensor data. As shown in Table~\ref{tab:benchmark_summary}, MongoDB achieves a substantially higher average insertion rate of $3783.46 \pm 551.40$ rows/sec, while PostgreSQL records $1785.05 \pm 274.34$ rows/sec. This result indicates that MongoDB sustains approximately twice the ingestion throughput of PostgreSQL under identical experimental conditions.

The distribution of throughput values is further illustrated in Fig.~\ref{fig:sub_insert_throughput}. MongoDB exhibits higher peak throughput but also larger variability, as reflected by its wider interquartile range and standard deviation. In contrast, PostgreSQL shows a more compact distribution, indicating more stable and predictable insertion behavior across runs.

From a system design perspective, MongoDB benefits from its document-oriented storage engine and optimized bulk insert pipeline, which reduces per-record transaction overhead. PostgreSQL with TimescaleDB, while offering stronger transactional guarantees and durability through write-ahead logging, incurs higher synchronization and write amplification costs. For consumer electronics and IoT image streaming applications, these results suggest that MongoDB is better suited for high-speed data ingestion, whereas PostgreSQL offers superior stability and consistency.


\subsection{Aggregation Query Performance}
\label{subsec:aggregation_latency}

Aggregation latency reflects the efficiency of analytical processing over stored time-series image data. As reported in Table~\ref{tab:benchmark_summary}, PostgreSQL significantly outperforms MongoDB in aggregation performance, achieving an average latency of $23.97 \pm 0.82$ ms, while MongoDB records a much higher latency of $353.13 \pm 16.53$ ms. This result demonstrates an order-of-magnitude advantage for PostgreSQL in executing time-bucketed aggregation queries.

The performance distribution is visualized in Fig.~\ref{fig:sub_aggregation_latency}. PostgreSQL exhibits both low median latency and a narrow interquartile range, indicating highly consistent analytical performance across repeated runs. In contrast, MongoDB shows substantially higher latency with wider dispersion, reflecting greater execution variability caused by its document-scanning and pipeline-based aggregation mechanism.

This performance gap is primarily attributed to PostgreSQL’s query optimizer, columnar-style access patterns in TimescaleDB, and its native support for time-based partitioning via hypertables. These features enable efficient chunk pruning and index utilization during aggregation. MongoDB, while flexible in schema design, processes aggregations through its document pipeline engine, which introduces higher computational overhead for large-scale time-bucket operations. For consumer electronics and IoT workloads requiring real-time or near-real-time analytics, PostgreSQL therefore provides a clear advantage in aggregation performance.


\subsection{Driver Roundtrip Overhead}
\label{subsec:driver_overhead}

Driver roundtrip overhead represents the baseline communication and parsing cost between the application and the database engine, independent of query complexity. As shown in Table~\ref{tab:benchmark_summary}, PostgreSQL exhibits a very low average roundtrip latency of $0.0415 \pm 0.0360$ ms, while MongoDB records $0.0858 \pm 0.0030$ ms. Both values are in the sub-millisecond range, indicating that driver communication overhead is negligible when compared to insertion and aggregation costs.

Although MongoDB shows approximately twice the roundtrip latency of PostgreSQL in relative terms, the absolute difference remains extremely small and does not materially affect end-to-end system performance for bulk ingestion or analytical workloads. The dominant performance factors therefore remain disk I/O, write amplification, indexing, and query execution rather than client–server communication latency. This confirms that the throughput and aggregation benchmarks primarily reflect backend engine behavior rather than client library inefficiencies.



\subsection{Storage Utilization Analysis}
\label{subsec:storage_size}

Storage efficiency is a critical factor for large-scale image-based time-series systems, particularly in consumer electronics environments with limited storage capacity. As reported in Table~\ref{tab:benchmark_summary}, PostgreSQL requires $4777.72$ MB for the table and $4787.29$ MB at the database level, while MongoDB consumes only $835.41$ MB for the collection and $835.45$ MB for the database. This indicates that MongoDB uses approximately six times less storage than PostgreSQL for the same dataset.

This substantial difference is primarily attributed to MongoDB’s built-in compression mechanisms in the WiredTiger storage engine, which apply block-level compression to both data and indexes. In contrast, PostgreSQL stores large binary objects as uncompressed \texttt{BYTEA} values within table and TOAST storage structures. As a result, although both systems store identical image content at the logical level, MongoDB achieves a significantly more compact physical representation on disk.

From a system design perspective, these results suggest that MongoDB is better suited for storage-constrained IoT and consumer electronics deployments where minimizing on-device storage footprint is essential. PostgreSQL, while incurring higher storage overhead, offers stronger transactional guarantees and mature relational indexing support. The choice between the two systems therefore involves a clear trade-off between storage efficiency and relational robustness in large-scale image time-series applications.


\subsection{Overall System Trade-offs}
\label{subsec:overall_tradeoffs}

The experimental results reveal clear and complementary trade-offs between PostgreSQL and MongoDB across the three primary performance dimensions of this study: insertion throughput, aggregation latency, and storage utilization. MongoDB consistently demonstrates superior insertion throughput and significantly lower storage footprint, making it highly efficient for high-rate image ingestion and storage-constrained environments. In contrast, PostgreSQL exhibits substantially lower aggregation latency and more stable analytical performance, indicating its strength in query-intensive and real-time analytical workloads.

When interpreted in terms of frame rate requirements for consumer electronics (CE) devices, the insertion throughput directly determines the maximum sustainable frames per second (FPS). At a resolution of $320 \times 240$, MongoDB achieves an average throughput of approximately $3783$ rows/sec, indicating that it can theoretically sustain several thousand image frames per second from distributed sources, making it suitable for multi-camera and high-speed sensing scenarios. PostgreSQL, sustaining approximately $1785$ rows/sec, remains fully adequate for typical CE frame rates of $10$–$60$ FPS per device, but offers a smaller margin for high-density, multi-stream deployments. Conversely, the significantly lower aggregation latency of PostgreSQL enables near-real-time frame-level analytics, which is critical for applications such as video surveillance, quality inspection, and anomaly detection. From a system design perspective, MongoDB is therefore well suited for edge-level frame ingestion where high FPS and compact storage dominate, whereas PostgreSQL is better positioned for backend analytics where fast temporal aggregation and stable query performance are required.


\section{Conclusions}
\label{sec:conclusions}

This study presented a comprehensive experimental comparison between PostgreSQL with TimescaleDB and MongoDB for image-based time-series workloads representative of consumer electronics and IoT systems. The results demonstrated that MongoDB provides higher insertion throughput and significantly better storage efficiency due to its compressed document-oriented storage model, while PostgreSQL achieves substantially lower aggregation latency and more stable analytical performance through time-based partitioning and mature query optimization. When interpreted in terms of frame rate and image resolution, both systems are capable of supporting real-time ingestion for low-resolution streams, while exhibiting different scalability characteristics at higher resolutions. These findings confirm that database selection should be guided by application priorities across ingestion speed, analytical latency, and storage constraints. As future work, this study will be extended to evaluate performance under multiple concurrent client connections, parallel ingestion and query processing, and higher-resolution image streams (e.g., VGA, HD, and 720p) to better reflect real-world multi-camera consumer electronics deployments.


\section*{Acknowledgment}
This research was supported by Universitas Pradita and Universitas Multimedia Nusantara.


\bibliographystyle{IEEEtran}
\bibliography{references}

\end{document}